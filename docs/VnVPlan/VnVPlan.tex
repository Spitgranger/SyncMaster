\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  citecolor=blue,
  filecolor=black,
  linkcolor=red,
  urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{longtable}
\usepackage{enumitem,amssymb}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}

\input{../Comments}
\input{../Common}

\setcitestyle{numbers}

\begin{document}

\title{System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
  \toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
  \midrule
  Date 1 & 1.0 & Notes\\
  Date 2 & 1.1 & Notes\\
  \bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
  However, this does not mean listing every verification and
  validation technique
  that has ever been devised.  The VnV plan should also be a \textbf{feasible}
  plan. Execution of the plan should be possible with the time and
  team available.
  If the full plan cannot be completed during the time available, it
  can either be
  modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
  the design stage.  This means that the sections related to unit testing cannot
  initially be completed.  The sections will be filled in after the design stage
  is complete.  the final version of the VnV plan should have all
  sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\section*{Symbols, Abbreviations, and Acronyms}

Refer to \textit{Section 4 Naming Convnetions and Terminology} in the
SRS document\citep{SRS} for all relevant symbols, abbreviations, and acronyms.

\newpage

\pagenumbering{arabic}

\section{Overview}

This document outlines the Verification and Validation plan which will be used
to ensure the SyncMaster application meets the requirements specification
and the City of Hamilton's acceptance. The verification plan is specified
in detail outlining what methods will be used to verify the
functional and non-functional
requirements. The system tests to support this process are specified in detail.
The validation plan to ensure stakeholder acceptance is further identified.

\section{General Information}

\subsection{Summary}

The software being tested is `SyncMaster', an application being
developed for the City of Hamilton, Water Division. The general
functions of the application are:

\begin{itemize}
  \item Document Management: The application will have a document
    hosting functionality, allowing access based document management.
    It will also be able to retrieve and display the appropriate file
    depending on the situation, and allow the user to sign it if applicable.
  \item Work Order Portal: The application will have a portal
    accessible to those coming on site to complete contract work or
    work orders. The portal will allow users to verify they have
    completed the necessary health and safety trainings and are aware
    of any hazards they may face. The portal will also collect
    information from the user regarding the work being done, as well
    as show them relevant documentation.
  \item Displaying and Exporting Data: Data is collected through the
    portal, including additional data such as tracking the arrival
    and departure of contractors, the starting and completion time of
    work, and any documentation submitted by users. This data is
    available to be viewed and exported by users with the correct
    level of access.
  \item Permission Based Access: Users registered on the system will
    have different access to features depending the permissions given
    to their account type.
  \item Geolocation Verification: The application will be able to
    verify if a user is at the appropriate site using GPS.
  \item Notifications: The application will be able to notify users
    when certain events take place, errors occur, or misuse is detected.
\end{itemize}

\subsection{Objectives}

The objectives of this document are to be able to build confidence in
the correctness of the application being built, and to demonstrate
adequate usability of the application by performing the tests
outlined in this document.

It is out of scope for the verification and validation plans outlined
in this document to test external libraries. Instead it is assumed
that the implementation team for the external library has already
validated the library.

\subsection{Challenge Level and Extras}

The challenge level for this project is a general level, and the
extras are conducting user testing and developing a user manual. For
more information refer to the Problem Statement and Goals
\citep[\textit{Challenge Level and Extras}]{ProblemStatement}.

\subsection{Relevant Documentation}

\begin{enumerate}
  \item Problem Statement and Goals \citep{ProblemStatement}:
    Provides context on the goals and scope of the application.
  \item Software Requirements Specification \Citep{SRS}:
    Specification of all requirements being tested and validated in
    this document.
  \item Hazard Analysis \Citep{HazardAnalysis}: Additional
    specifications for security and safety requirements.
  \item Design Documents \Citep{MG, MIS}: Reference for the
    mechanisms and design behind functionality and systems being tested.
\end{enumerate}

\section{Plan}

This section outlines our plans for verifying and validating the
software under development, along with its accompanying
documentation. It provides an overview of the validation team and
details the approach to verifying the software's documentation,
design, and implementation, as well as validating the final product.

\subsection{Verification and Validation Team}

The verification and validation team will consist of members from the
development team as well as members from the project stakeholders. A summary of
the members and their roles are given in the table below.
\setlength{\arrayrulewidth}{0.5mm}
\setlength{\tabcolsep}{18pt}
\renewcommand{\arraystretch}{1.5}\\
\begin{tabular}{ | m{5cm} | m{9cm} | }
  \hline
  \textbf{Names and Role} & \textbf{Responsibilites} \\
  \hline
  \textbf{Mitchell Hynes, Kyle D'Souza, Richard Fan, Akshit Gulia, Rafeed
  Iqbal} - Development Team &
  \begin{itemize}
    \item Responsible for verification of implementation details and
      adherance of the system to the SRS. The development team will own the
      most of the verification activities pertaining to the software. This
      includes activities such as developing verification plans, code reviews,
      unit testing, integration testing, and SRS verification.

  \end{itemize}\\
  \hline
  \textbf{Matthew Yakymyshyn} - Stakeholder from the City of Hamilton &
  \begin{itemize}
    \item Responsible for the verification of the system. They will be
      responsible for ensuring, and determining to what degree the
      system fulfils its intended purpose.
      Some examples of the activities that they would be responsible for
      would be SRS validation and acceptance testing.

  \end{itemize}\\
  \hline
  \textbf{Tarnveer Takhtar, Matthew Bradbury, Harman Bassi, Kyen So}
  - Peer Reviewers &
  \begin{itemize}
    \item Responsible for critiquing and providing suggestions for artifacts
      produced. This would include reviewing design documents to ensure that
      they are complete and unambiguous. They will provide opinions and
      insights as third parties who are not directly involved in the project.
  \end{itemize}\\
  \hline
  \textbf{Spencer Smith, Yiding Li} - Teaching Team for 4G06 &
  \begin{itemize}
    \item They will also be responsible for the final
      evaluation of the system and will determine if the system meets the
      initially specified functional and non-functional requirements.
      They will also play a role in providing feedback on produced artifacts
      throughout the project.
  \end{itemize}\\
  \hline

\end{tabular}

\subsection{SRS Verification Plan}

To verify the SRS document, we will use the following methods:
\begin{enumerate}
  \item Formal reviews with our TA, Yiding. A checklist will be used to track
    the status of the review and be used as an instrument to verify our SRS.
    The checklist to be used is at the end of this section of the document.
  \item Ad-hoc peer reviews tracked through GitHub issues. Throughout the
    course we receive peer feedback on the quality of our SRS and VnV Plan. The
    peer feedback provided is valuable input which we will action through issues
    and pull requests as identified in the development plan.
  \item Milestone feedback provided through Avenue will also be used to
    improve the checklist which we use as our instrument to measure
    SRS verification.
\end{enumerate}
The checklist below will be used during formal reviews for SRS verification
with our TA.
\begin{itemize}
  \item Have the requirements been listed in the appropriate sections of the
    SRS document?
  \item Are all definitions and acronyms defined in the glossary of terms?
  \item Are the system inputs properly specified?
  \item Are the system outputs properly specified?
  \item Do the functional requirements avoid conflicts with other requirements?
  \item Do the functional requirements avoid specifying the system design?
  \item Are the requirements clear enough that they could be implemented by an
    independent engineering team correctly?
  \item Is each requirement testable?
  \item Is independent testing of each requirement possible?
  \item Are the functional requirements traceable to a use case of the system?
  \item Are there any open issues from reviewers on a requirement?
\end{itemize}

\subsection{Design Verification Plan}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}

To ensure the quality and completeness of the verification and
validation plan, the testing team will ensure the following:

\begin{enumerate}
  \item There will be peer reviews of the document coming from our
    assigned Peer Review team, to provide feedback on the plan.
  \item The TA assigned to the project will review and provide
    feedback on the plan when grading.
  \item The development team will review the document to ensure the
    quality of the plan.
  \item It should be checked by the testing team that each
    requirement has a test case associated with it. This can be done
    using traceability matrices and cross-referencing requirements
    with what exists in the SRS.
\end{enumerate}

The following is a checklist that can be used to verify the
verification and validation plan:
\begin{todolist}
\item Ensure all issues on the project GitHub related to the document
  from the Peer Review team is closed.
\item Ensure that all feedback provided by the TA in the rubric for
  the document on Avenue is addressed.
\item Ensure the Development team has reviewed the document and is in
  agreement that the document is up to quality and completeness standards.
\item Check the traceability matrices to ensure that all requirements
  have a test case associated with them.
\end{todolist}

\subsection{Implementation Verification Plan}

\begin{itemize}
  \item To ensure proper implementation of application functionality,
    unit tests will be conducted on all functions and modules as
    specified in this document.
  \item Continuous Integration (CI) tools will be utilized to
    automatically test all code before deployment, verifying code
    integrity and identifying issues early in the development process.
  \item All pull requests will undergo a detailed review process to
    ensure code quality, adherence to standards, and functional
    accuracy before merging into the main codebase.
  \item Following each revision, a team code walkthrough will be
    conducted to review changes collectively, fostering knowledge
    sharing and alignment on implementation details.
\end{itemize}

\subsection{Automated Testing and Verification Tools}
\begin{itemize}
  \item Python unit testing: PyTest
  \item JavaScript/TypeScript unit testing: Jest
\end{itemize}
Detailed information about automated testing and verification tools
that we plan to use are explained in the Development Plan
\citep[\textit{Expected Technology, Coding Standard}]{ProblemStatement}.
In terms of quality metrics, we plan on using GitHub actions to
generate a report
every time a pull request is made against the main branch. The report
will be generated from the unit testing framework selected, more
specifically jest-html-reporter and pytest-cov. Initially, these tests
will include unit tests with more types of automated testing to be determined as
the project progresses.

\subsection{Software Validation Plan}

\begin{enumerate}
  \item Rev0 prototype demonstration to Technical Services staff. The rev0
    prototype will be demonstrated to the City Staff on this team at
    an in-person
    meeting. Each requirement from the SRS will be demonstrated in
    the demo so the
    full scope of the application is displayed. The demonstration will also show
    how it satisfies the use cases identified in the SRS. Feedback
    received from the
    City at this meeting will be used to improve the prototype for rev1.

  \item Stakeholder progress check-ins. Short periodic meetings will be arranged
    with the Technical Services team as required to demonstrate the
    user interface
    design prototypes and receive feedback from the facilities
    managers. Email updates
    will also be sent for items which require stakeholder
    clarification. The user interface is the most important part of
    the application to validate with the end-users.
    Consistent communication on the direction of the interface design
    to ensure it
    is usable will catch problems early and greatly improve the quality and
    acceptance of the application.
\end{enumerate}

\section{System Tests}

\subsection{Tests for Functional Requirements}

\subsubsection{File Handling}

\begin{enumerate}

  \item{TC-FR1\\}

    Control: Manual

    Initial State: The system is running and user is logged in

    Input: A file (can have any extension)

    Output: File should be uploaded successfully and visible in the folder/path
    it was uploded under with the following details:
    \begin{enumerate}
      \item File Name
      \item Date Modified
      \item Type
      \item Size
      \item Owner
    \end{enumerate}

    Test Case Derivation: The system is expected to support the upload of files
    regardless of their file types and also show the details mentioned above
    once they are successfully uploaded.

    How test will be performed: Files with common file types
    (.pdf, .docx, .xlsx, .txt, .png, .jpeg, .jpg, .zip and .rar
    will be used for testing) and uncommon file types (.rb, .rda, .bin and .vdi
    will be used for testing) will be manually uploaded. Afterwards, their
    visibility on the portal will be verified.

  \item{TC-FR2\\}

    Control: Manual

    Initial State: The system is running, and the user is logged in

    Input: A file present in the portal (i.e., an uploaded file)

    Output: The user should able to view the file in the portal if it is one
    of the following types:
    \begin{enumerate}
      \item .doc
      \item .docx
      \item .xls
      \item .xlsx
      \item .ppt
      \item .pptx
      \item .pdf
      \item .csv
      \item .txt
    \end{enumerate}
    If the file is not one of the above types, then the message \lq\lq File
    cannot be viewed in the portal. Download file to view it." should be
    displayed.

    Test Case Derivation: The above mentioned types are commonly used by the
    stakeholders in day-to-day operations and therefore, they should be able to
    view them. Less common file types (ones not mentioned above) are not used by
    our stakeholders. Moreover, file types like .xlsm (macro-enabled excel
    workbook) can be malcious and thus, can compromise the safety of the system
    (malcious macro present in a macro-enabled excel workbook executes when file
    is opened/viewed). Therefore, they are not viewable on the portal.

    How test will be performed: Two random files, one having the file type
    supported for viewing (as mentioned above), and other having an unsupported
    file type will be selected and viewed.

\end{enumerate}

\subsubsection{Access Control}

\begin{enumerate}
  \item {TC-FR3\\}

    Control: Manual

    Initial State: The system is running and different user roles of Admin,
    Contractor/General user exist.

    Input: User Role (Admin, Contractor/General user)

    Output: Admin users should be able to change permissions and access all
    documents. Contractor/General users should only have read access and the
    ability to sign documents.
    Test Case Derivation: The system should enforce different access levels as
    specified in the functional requirements in the SRS document.

    How test will be performed: Manually log in with different users roles (
    as mentioned above) and verify the access permissions and capabilities.

\end{enumerate}

\subsubsection{User Information Visibility}

\begin{enumerate}
  \item {TC-FR4\\}

    Control: Manual

    Initial State: The system is running and the number of users in the system
    is greater than or equal to 2 and there is atleast one admin user and
    atleast one contractor/general user.

    Input: A valid user id

    Output: Admin users should be able to view all the details of the other user
    with the corresponding user id provided.

    Test Case Derivation: Admin users need to access contractor information to
    manage work orders and job details. In some cases, admin users will also
    need to access details of other admin users.

    How test will be performed: Manually log in as an admin user and verify the
    visibility of the user details based on the user id provided.

\end{enumerate}

\subsubsection{Geolocation Verification}

\begin{enumerate}
  \item {TC-FR5\\}

    Control: Manual

    Initial State: The system is running and the location service is enabled
    on the contractor's device

    Input: Contractor user's location

    Output: Authentication and signing/uploading of the document should be
    allowed if within the allowed geolocation. Otherwise, denied.

    Test Case Derivation: To ensure presence of contractors on the plant, the
    should restrict the actions mentioned above if they are not within premises
    of the plant.

    How test will be performed: Manually attempt to authenticate and sign/upload
    documents from different locations and verify the results.

\end{enumerate}

\subsubsection{Notifications}

\begin{enumerate}
  \item {TC-FR6\\}

    Control: Manual

    Initial State: The system is running, and compliance documents are present
    in the system

    Input: Expired compliance document

    Output: The user should receive a notification when a compliance document
    expires

    Test Case Derivation: Notifications for expired documents ensure users stay
    compliant and up-to-date.

    How test will be performed: Manually expire a compliance document and verify
    that the user receives a notification.

  \item {TC-FR7\\}

    Control: Manual

    Initial State: The system is running, and there are contractor users who
    have not completed required training.

    Input: Contractor user without completed training.

    Output: The manager of the contractor should receive a notification when the
    contractor attempts to authenticate and upload documents without required
    training.

    Test Case Derivation: Managers need to be aware of contractor's traning
    statuses to ensure compliance.

    How test will be performed: Manually attempt to authenticate and upload
    documents without completing required training as a contractor user and
    verify that the manager is notified.

\end{enumerate}

\subsubsection{Document Acknowledgement}

\begin{enumerate}
  \item {TC-FR8\\}

    Control: Manual

    Initial State: The system is running, and documents are available for
    acknowledgement.

    Input: Document for acknowledgement.

    Output: The should store the date, time, document name and name of the
    person who signed the acknowledgement document.

    Test Case Derivation: Accurate record-keeping is required for
    accountability and traceability.
    How test will be performed: Manually acknowledge a document and verify that
    the system stores the correct information.

\end{enumerate}

\subsubsection{Training Verification}

\begin{enumerate}
  \item {TC-FR9\\}

    Control: Manual

    Initial State: The system is running, and there are contractor users who
    have not completed the required training.

    Input: Contractor user without completed training

    Output: The should prevent a contractor user from authenticating or
    uploading a document if the required training is not completed.

    Test Case Derivation: Contractors are required to complete training before
    they start their work at the plant.

    How test will be performed: Manually try to authenticate and upload document
    as a contractor user without completing required training and verify that
    the system prevents it.

\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
necessarily have nonfunctional requirements related to accuracy.}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
  passing the test, but rather describing the experiment you will do to measure
  the quality for different inputs.  For instance, you could measure
  speed versus
  the problem size.  The output of the test isn't pass/fail, but
  rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
  How will they be done?  In cases like code (or document)
  walkthroughs, who will
be involved? Be specific.}

\subsubsection{Usability and Humanity Requirements}

\paragraph{Ease of Use}

\begin{enumerate}

  \item{TC-EU-1\\}

    Type: Dynamic, Manual

    Initial State: System is functional with test accounts made for users. Users
    are between $\hypertarget{min_age}{MIN\_AGE}$ and
    $\hypertarget{min_age}{MIN\_AGE}$ age limits. No prior instructions or
    training are provided.

    Input/Condition: User who has no prior experience using the system attempts
    to use it.

    Output/Result: Users are able to discover at least 70\% of the main
    functionalities of the system within 10 minutes without assitance.

    How test will be performed: The test will be performed by letting users from
    our stakeholder who have no prior knowledge of the system use the system.
    After using the system they will fill out a short quiz that asks them
    about the specific features/uses of the system. Scoring above 70\% is
    considered a pass. If 70\% or more of testers pass, we can say that this
    test passes. See \textbf{Section 6.2} Usability Survey Questions.

  \item{TC-EU-2\\}

    Type: Dynamic, Manual

    Initial State: System is functional with test accounts made for users. Users
    are between $\hypertarget{min_age}{MIN\_AGE}$ and
    $\hypertarget{min_age}{MIN\_AGE}$ age limits.

    Input: Users logs in, and performs an action classified as irreversible
    (e.g. deletion).

    Output: The user sees a notification informing them that their action is
    irreversible or an undo option.

    How test will be performed: The test will be performed by a user from our
    stakeholder. The user will be instructed to perform an irrversible action on
    the system. After performing the action and viewing the prompt, the user
    will be asked if they understood the implications of their actions. If the
    user understands the prompt and actions correctly, the test is considered
    successful.

\end{enumerate}

\paragraph{Learning}
\begin{enumerate}
  \item{TC-LR-1\\}

    Type: Dynamic, Manual

    Initial State: System is functional with test accounts made for users. Users
    are between $\hypertarget{min_age}{MIN\_AGE}$ and
    $\hypertarget{min_age}{MIN\_AGE}$ age limits.

    Input: User logs in, and uses a feature that they have not used
    before (e.g. viewing documents)

    Output: User has explored the new feature and is confident in using the
    feature.

    How test will be performed: The test will be performed by a user from our
    stakeholder. The user will be instructed to use a feature of the system that
    they have not seem before. After 10 minutes, the user will be asked to rate
    their understanding of the feature and how confident they are using it. The
    questions will be a simple 1-10 scale. If the user selects a 7 or above, the
    test is considered passed. See \textbf{Section 6.2} Usability
    Survey Questions.

  \item{TC-LR-2\\}

    Type: Dynamic, Manual

    Initial State: System onboarding documents are avaliable. Users
    are between $\hypertarget{min_age}{MIN\_AGE}$ and
    $\hypertarget{min_age}{MIN\_AGE}$ age limits.

    Input: User views the system onboarding documents.

    Output: User has finished viewing the system onboarding documents and
    reports their confidence on how they feel about using the system.

    How test will be performed: The test will be performed by a user from our
    stakeholder. They will be asked to view the onboarding documents. After 5
    minutes the user will be asked a few questions to gauge their
    confidence when using the
    system. See \textbf{Section 6.2} Usability Survey Questions.

\end{enumerate}

\paragraph{Understandability and Politeness}
\begin{enumerate}
  \item{TC-UP-1\\}

    Type: Dynamic, Manual

    Initial State: System is functional with test accounts made for users. Users
    are between $\hypertarget{min_age}{MIN\_AGE}$ and
    $\hypertarget{min_age}{MIN\_AGE}$ age limits.

    Input: User logs in, and views all available screens of the user interface.

    Output: The user has seen the images and text on all available screens of
    the user interface and notes the content seen.

    How test will be performed: The test will be performed by a user from our
    stakeholder. They will be guided through all screens for every type of role
    (General/Contractor, Manager, Admin). They will then be asked if any of the
    images or text seen contains anything that they might have found offensive
    or politically charged. If the user reports they haven't seen anything that
    may have been offensive or poitically charged, the test is successful.

  \item{TC-UP-2\\}

    Type: Dynamic, Manual

    Initial State: System is functional with test accounts made for users. Users
    are between $\hypertarget{min_age}{MIN\_AGE}$ and
    $\hypertarget{min_age}{MIN\_AGE}$ age limits.

    Input: User performs an action that puts the system into an error state
    (e.g. deleting a document that they don't have permission to modify).

    Output: User has performed the actions and the system has informed the user
    of the error. User actions the feedback given and understands the potential
    resolution paths.

    How test will be performed: The test will be performed by a user from our
    stakeholder. The user will be instructed to perform an action that will put
    the system in an error state. They will then perform the appropriate action
    after receiving the feedback from the system. The user will then be asked if
    they understood the error and if they were able to figure the possible
    solutions. If the user reports that they have a clear understanding, the
    test is successful.

\end{enumerate}

\paragraph{Accessibility}
\begin{enumerate}
  \item{TC-AS-1\\}

    Type: Dynamic, Manual

    Initial State: System is functional with test accounts made for users. Users
    are between $\hypertarget{min_age}{MIN\_AGE}$ and
    $\hypertarget{min_age}{MIN\_AGE}$ age limits.

    Input: User logs in, and views all available screens of the user interface.

    Output: The user seen all available screens of
    the user interface and notes the content seen.

    How test will be performed: The test will be performed by a user from our
    stakeholder. They will be guided through all screens for every type of role
    (General/Contractor, Manager, Admin). They will then be asked if the design
    of the user interface aligns with accessibility features that they have
    seen in other applications used by the city. If the user reports that the
    accessibility features seen are similar to other applications that
    they already use, the test is successful.

\end{enumerate}

\subsubsection{Look and Feel Requirements}

\paragraph{Appearance}

\begin{enumerate}

  \item{TC-LF-1\\}

    Type: Manual

    Initial State: The system is in an operational state, with all
    components running

    Input/Condition: User logs in, and views all available screens of
    the user interface

    Output/Result: The user has seen all available screens of the
    user interface and finds that most of the colours used by the
    application match a similar colour palette to existing
    applications used by the city of Hamilton.

    How test will be performed: The test will be performed by getting
    one user who works at the city of Hamilton for each type of role
    (General/Contractor, Manager, Admin). Then a member the
    development team will guide them to all available screens for their role and
    ask them to evaluate how many colours that were seen are were
    similar to ones used in other applications used by the city of
    Hamilton and how many were not similar. If the number of colours
    that are similar is larger than the number that are not, then the
    test is successful, otherwise it is failed.

  \item{TC-LF-2\\}

    Type: Dynamic, Manual.

    Initial State: The system is in an operational state, with all
    components running

    Input/Condition: Load and navigate through the application on
    various supported operating systems, with varying supported
    browsers and screen sizes.

    Output/Result: User is able to interact with the system on all screen
    sizes and understand the interface regardless of screen size

    How test will be performed: One user for each type of role
    (General/Contractor, Manager, Admin), will be go through the core
    functionalities important for their roles and ensure that all
    functionalities work as expected across screen sizes, browsers,
    and operating systems. The tests should be performed using
    Windows 10, IOS, and Android, which are the supported operating
    systems for this application. The browsers used should be
    Microsoft Edge and Google Chrome. Testing should include a mobile
    device, a desktop, and a tablet to ensure accommodation of
    different screen sizes and of different types of devices.

    The core functionalities for a contractor/general user will be
    signing into the system, viewing a document, signing a document,
    and logging out. The core functionalities of a manager will be
    uploading a document, viewing document update history, and seeing
    the status of a contractor/general user under them. The core
    functionalities of an admin will be creating users, deleting
    users, and assigning roles to users.

\end{enumerate}

\subsubsection{Maintainability and Support Requirements}

\paragraph{Maintenance}

\begin{enumerate}

  \item{TC-MS-1\\}

    Type: Manual

    Initial State: Cloud platform used for deployments is functional,
    GitHub actions is functional.

    Input/Condition: A workflow run in GitHub actions is triggered to
    deploy the application into a specified environment, and
    building/automated unit testing of the application is completed

    Output/Result: The application is deployed to the specified
    environment in under 30 minutes

    How test will be performed: The test will be performed by
    triggering a GitHub actions workflow run to deploy the application
    into the development environment, and waiting for it to complete.
    Once completed, the unit testing and building time will be subtracted
    from the total runtime of the workflow run, to determine the
    deployment time, this should be under 30 minutes to be considered a success.

  \item{TC-MS-2\\}

    Type: Manual

    Initial State: GitHub actions is functional.

    Input/Condition: A workflow run in GitHub actions is triggered to
    deploy the application into a specified environment, and
    automated unit testing of the application is completed.

    Output/Result: The application is built in under 10 minutes.

    How test will be performed: The test will be performed by
    triggering a GitHub actions workflow run to deploy the application
    into the development environment, and waiting for it to complete.
    Once completed, the unit testing and deployment time will be subtracted
    from the total runtime of the workflow run, to determine the
    build time, this should be under 10 minutes to be considered a success.

  \item{TC-MS-3\\}

    Type: Manual

    Initial State: GitHub actions is functional.

    Input/Condition: A workflow run in GitHub actions is triggered to
    perform all automated testing (end-to-end or unit testing) of the
    application.

    Output/Result: The automated tests for the application run in
    under 10 minutes.

    How test will be performed: The test will be performed by
    triggering a GitHub actions workflow run to perform all automated
    tests, and checking that once this run is completed, the total
    runtime is under 10 minutes.

  \item{TC-MS-4\\}

    Type: Manual

    Initial State: GitHub actions is functional.

    Input/Condition: A push is made to a branch with a pull request
    open off of it.

    Output/Result: The branch is found to have line coverage $\ge$ 95\% and
    the branch coverage is found to be $\ge$ 90\%

    How test will be performed: The test will be performed by
    triggering a GitHub actions workfow run to run unit tests on the
    application upon every push to a branch with a PR open off of it.
    The workflow will generate a coverage report, and when the
    coverage report is generated, it should be checked that the line
    coverage and branch coverage meet the expectations.

  \item{TC-MS-5\\}

    Type: Manual

    Initial State: GitHub is functional

    Input/Condition: The GitHub repository for the project is checked
    to ensure that all functional requirements listed in the SRS have
    a unit and end-to-end test cases corresponding to them.

    Output/Result: All functional requirements found in the SRS have a unit
    and end-to-end test cases corresponding to them

    How test will be performed: The test will be performed by
    checking the Traceability Matrices in this document, to make sure
    that there exists a unit and end-to-end test case for all
    functional requirements.

  \item{TC-MS-6\\}

    Type: Manual

    Initial State: GitHub is functional

    Input/Condition: The GitHub repository for the project is checked
    to ensure that all appropriate documentation existed for users to
    be able to maintain the system.

    Output/Result: Instructions are provided in the GitHub repository
    for the project on how users can continue to maintain the system.
    This includes contribution guidelines, descriptions of all
    GitHub actions workflows, and documentation on the design of the system.

    How test will be performed: The test will be performed by
    checking that the documentation listed in the output/result
    exists in the repository, and by verifying with the Matthew
    Yakymyshyn that this documentation is able to be understood by
    the city of Hamilton.

  \item{TC-MS-7\\}

    Type: Manual

    Initial State: GitHub is functional

    Input/Condition: The GitHub repository for the project is checked
    to see the user manual.

    Output/Result: There exists a user manual in the github
    repository which describes, at a minimum, how to leverage the
    functionalities described in the functional requirements of the
    SRS from the user interface.

    How test will be performed: The test will be performed by
    checking that the user manual exists, and that there is
    user-facing documentation in the manual on how to achieve all
    functionalities described in the functional requirements.

  \item{TC-MS-8\\}

    Type: Manual

    Initial State: GitHub is functional

    Input/Condition: The GitHub repository for the project is checked
    to see the API documentation.

    Output/Result: There exists API documentation in the github
    repository which follows the OpenAPI Specification (OAS) standard.

    How test will be performed: The test will be performed by
    checking that an OpenAPI Specification for the API's provided by
    the system exists on the project repository.

  \item{TC-MS-9\\}

    Type: Manual

    Initial State: GitHub is functional

    Input/Condition: The GitHub repository for the project is checked
    to see the the internal documentation.

    Output/Result: There exists documentation on all internal
    functions and classes defined in the project repository.

    How test will be performed: The test will be performed by
    checking that documentation exists for all functions and classes
    defined in the project repository.

  \item{TC-MS-10\\}

    Type: Manual

    Initial State: GitHub is functional, Cloud deployment platform is functional

    Input/Condition: The GitHub repository for the project is checked
    to see the the deployment documentation

    Output/Result: There exists documentation on how to deploy the
    project, which is up-to-date and works when attempted.

    How test will be performed: The test will be performed by
    checking that the deployment documentation exists and that a
    member of our development team can follow the provided steps to
    successfully deploy the application.

\end{enumerate}

\subsubsection{Compliance Requirements}

\paragraph{Compliance}

\begin{enumerate}

  \item{TC-L-1\\}

    Type: Manual

    Initial State: Github is functional, cloud deployment platform is functional

    Input/Condition: A contractor uses the authentication feature of
    the application
    to verify their presence on site for the facilities managers.

    Output/Result: The contractor successfully authenticates in the application
    and successfully complies with each step of the entry/exit
    procedure document
    for the station provided through the application.

    How the test will be performed: The contractor user will authenticate at the
    site for a work order to fix an exhaust fan. Through the
    authentication process,
    the entry/exit procedure will be used as a checklist to ensure it
    is followed
    and the legislation specified in CR-L1 is not violated.

  \item{TC-L-2\\}

    Type: Manual

    Initial State: Github is functional, cloud deployment platform is functional

    Input/Condition: The admin account registers a contractor account with the
    contractors personal email address.

    Output/Result: The contractors account is successfully
    registered, and the email
    address is only possible to view either from the facility managers
    account, the admin
    account, and the contractors account.

    How the test will be performed: The facility manager will use the
    interface to
    register an account in the application. The application will then
    be opened in
    different views based on profile. These profiles will be the
    registered contractor,
    the admin user, the facilities manager, and a second contractor profile.
    The test is successful when the email address of the contractor is only
    visible in the profile of that contractor, the facilities manager,
    and the admin user.

  \item{TC-L-3\\}

    Type: Manual

    Initial State: Github is functional, cloud deployment platform is functional

    Input/Condition: A facilities manager signs into the application and
    downloads a compliance document.

    Output/Result: This document is sent to the administrative team of the BCOS
    system who are able to record the document in their system.

    How the test will be performed: The facilities manager signs into the system
    and downloads a compliance document. The test is successful if this
    document is confirmed by the City it is in a form that is able to be
    uploaded to the BCOS system.

\end{enumerate}

\subsubsection{Cultural Requirements}

\paragraph{Cultural}

\begin{enumerate}
  \item{TC-CR-1\\}

    Type: Manual

    Initial State: Github is functional, cloud deployment platform is functional

    Input/Condition: The Github repository code is checked and the code for the
    display of the user interface is opened.

    Output/Result: The user interface and user experience is aligned with the 5
    City of Hamilton cultural pillars. These are Collective Ownership,
    Steadfast Integrity, Courageous Change, Sensational Service, and
    Engaged and Empowered Employees.

    How the test will be performed: The capstone team will compare the qualities
    of the user interface application code with the 5 pillars and assess whether
    each pillar is met by the application. The test passes if the
    team determines
    that the pillars are satisfied.
\end{enumerate}

\subsubsection{Operational and Environmental Requirements}

\paragraph{Ability to Connect and Use}

\begin{enumerate}

  \item{TC-OE-1\\}

    Type: Manual

    Initial State: Device connected to City of Hamilton network

    Input/Condition: Attempt to connect to the application using
    the network.

    Output/Result: The device successfully connects to and is able to
    use the application.

    How test will be performed: A user on site at the City of
    Hamilton, Water Division will connect to the network available to
    employees and contractors and then attempt to connect to and use
    the application. The test is successful if the application can be
    connected to using the city's network.

  \item{TC-OE-2\\}

    Type: Manual

    Initial State: Devices connected to the internet and have the
    supported browsers available.

    Input: Desktop Browsers being tested are Chrome and Edge, and the
    mobile devices being tested are Android and iPhone.

    Output: The devices successfully connect to and are able to use
    the application.

    How test will be performed: Exploratory testing using the
    supported devices and browsers. The devices should be able to
    connect and use all functionality in the application, and provide
    a similar experience.

\end{enumerate}

\paragraph{Future Integration}

\begin{enumerate}

  \item{TC-OE-3\\}

    Type: Manual

    Initial State: Branch with functionality to fill in data fields
    when provided a valid work order number

    Input/Condition: Valid work order number

    Output/Result: Relevant data fields should be filled in by
    the application

    How test will be performed: Dummy API used which returns work
    order information given a work order number. Will test if the
    functionality in the branch is able to fill in the fields with
    data from the API, indicating it is viable to add this
    integration in the future.

\end{enumerate}

\paragraph{Release and Change Log}

\begin{enumerate}

  \item{TC-OE-4\\}

    Type: Manual

    Initial State: New revision

    Input/Condition: Revision released, change log produced

    Output/Result: Revision is released by planned date, change log
    details all changes.

    How test will be performed: Whenever a revision is released, will
    inspect the change log to check if all changes have been noted.

\end{enumerate}

\subsubsection{Safety and Security Requirements}

\paragraph{Account Permissions}

\begin{enumerate}

  \item{TC-SS-1\\}

    Type: Manual

    Initial State: User IDs of each possible access level created.

    Input/Condition: Attempt to use every feature available using each ID

    Output/Result: Users only have access to features allowed by
    their permissions.

    How test will be performed: Accounts will be created on the
    application for each available level of access. Using each
    account, the use of each feature of the application will be
    attempted. Wether an action was possible or not will be noted and
    compared to the permissions of the account.

  \item{TC-SS-2\\}

    Type: Manual

    Initial State: User IDs of each possible access level created.

    Input/Condition: Attempt to upload, delete, modify files in
    different directories.

    Output/Result: Users only have access to files and directories
    allowed by their permissions.

    How test will be performed: Accounts will be created on the
    application for each available level of access. Using each
    account, the creation, deletion and modification of of files with
    varying permission requirements will be attempted. The testing
    will be done to cover directories of varying permission
    requirements. Wether an action was possible or not will be noted
    and compared to the permissions of the account.

\end{enumerate}

\paragraph{Data Validation}

\begin{enumerate}

  \item{TC-SS-3\\}

    Type: Manual

    Initial State: Initial set of files uploaded to application

    Input/Condition: View files on the application

    Output/Result: Files on the application are the appropriate
    version available on SharePoint and/or MySDS

    How test will be performed: Files will be checked against their
    current versions. Users with appropriate permissions should have
    the ability to update the files if outdated.

  \item{TC-SS-4\\}

    Type: Manual

    Initial State: Data fields empty.

    Input/Condition: Submit incomplete, impossible, or malicious data
    through data fields

    Output/Result: Fail-safes trigger, noting the bad data and
    informing the administrator.

    How test will be performed: Attempt to submit data fields filled
    in with a set of entries which vary between incomplete,
    impossible, and malicious and check how the application handles the inputs.

\end{enumerate}

\paragraph{Encryption}

\begin{enumerate}

  \item{TC-SS-5\\}

    Type: Manual

    Initial State: The application is accessed via a web browser with
    tools for inspecting network requests.

    Input/Condition: Access and use the application using https:// in a browser

    Output/Result: Network requests displayed as HTTPS in the
    browser's developer tools, and the security certificates are
    valid and properly configured.

    How test will be performed: Connect to and use the application
    using https:// and ensure the connection is secured with SSL/TLS.
    Use the browser's developer tools to inspect the requests made by
    the site. Check that all requests are made over HTTPS and the
    lock icon/secure status is visible in the browser's address bar.
    Check if SSL certificate is valid and correctly configured.

\end{enumerate}

\paragraph{Adherance to Policies, Regulations, and Laws}

\begin{enumerate}

  \item{TC-SS-6\\}

    Type: Manual

    Initial State: The application is complete, checklist(s)
    outlining all Federal, Provincial, Municipal and City of
    Hamilton, Water Division rules, regulations and policies created.

    Input/Condition: Review the application's functionality, data
    handling, and security measures against checklist(s)

    Output/Result: The application should align with all defined
    requirements, policies, and regulations, showing full compliance
    with legal and departmental standards without any deviations.

    How test will be performed: Develop a comprehensive checklist
    that includes all applicable government regulations, standards,
    and internal policies. Cross-reference the application's design
    and operational documentation with the compliance checklist.
    Perform a manual inspection of the application's source code,
    configuration settings, and security protocols to ensure they
    meet the required standards. Test relevant features to confirm
    that they adhere to the compliance checklist.

\end{enumerate}

\paragraph{Adherance to Policies, Regulations, and Laws}

\begin{enumerate}

  \item{TC-SS-7\\}

    Type: Manual

    Initial State: The application is being actively maintained.

    Input/Condition: A security vulnerability or weakness is discovered.

    Output/Result: The vulnerability or weakness is patched withing
    seven days of discovery.

    How test will be performed: Inspection of wether patch has
    resolved the weakness within the deadline

\end{enumerate}

\paragraph{Hazard Handling}

\begin{enumerate}

  \item{TC-SS-8\\}

    Type: Manual

    Initial State: Unsigned document and contractor user account
    exists in application

    Input/Condition: Contractor user declines signature, attempts to
    sign but does not complete action, or ignores document.

    Output/Result: Fail-safes trigger, noting the bad data and
    informing the facilities manager.

    How test will be performed: Attempt to submit without signature
    and check how the application handles the action.

\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
requirements.}

\begin{longtable}{|l|l|}
  \hline
  \textbf{Req. ID} & \textbf{Test ID's} \\
  \hline
  FR1 & TC-FR1, TC-FR2\\ \hline
  FR3 & TC-FR3\\ \hline
  FR4 & TC-FR4\\ \hline
  FR5 & TC-FR5\\ \hline
  FR6 & TC-FR6\\ \hline
  FR7 & TC-FR8\\ \hline
  FR8 & TC-FR7\\ \hline
  FR9 & TC-FR9\\ \hline
  LF-AP1 & TC-LF-1 \\ \hline
  LF-ST1 & TC-LF-2 \\ \hline
  UH-EU1 & TC-EU1\\ \hline
  UH-EU2 & TC-EU2\\ \hline
  UH-LR1 & TC-LR1\\ \hline
  UH-LR2 & TC-LR2\\ \hline
  UH-UP1 & TC-UP1\\ \hline
  UH-UP2 & TC-UP2\\ \hline
  UH-AS1 & TC-AS1\\ \hline
  PR-SL1 & \\ \hline
  PR-SL2 & \\ \hline
  PR-SL3 & \\ \hline
  PR-SC1 & \\ \hline
  PR-SC2 & \\ \hline
  PR-PA1 & \\ \hline
  PR-PA2 & \\ \hline
  PR-RFT1 & \\ \hline
  PR-RFT2 & \\ \hline
  PR-CR1 & \\ \hline
  PR-CR2 & \\ \hline
  PR-SE1 & \\ \hline
  PR-SE2 & \\ \hline
  PR-LR1 & \\ \hline
  OE-PE1 & TC-OE-1 \\ \hline
  OE-WE1 & TC-OE-2 \\ \hline
  OE-WE2 & TC-OE-2 \\ \hline
  OE-IAS3 & TC-OE-3 \\ \hline
  OE-REL1 & TC-OE-4 \\ \hline
  OE-REL2 & TC-OE-4 \\ \hline
  OE-REL3 & TC-OE-4 \\ \hline
  OE-REL4 & TC-OE-4 \\ \hline
  MS-MTN1 & TC-MS-1 \\ \hline
  MS-MTN2 & TC-MS-2 \\ \hline
  MS-MTN3 & TC-MS-3 \\ \hline
  MS-MTN4 & TC-MS-4 \\ \hline
  MS-MTN5 & TC-MS-5 \\ \hline
  MS-MTN6 & TC-MS-6 \\ \hline
  MS-SUP1 & TC-MS-7 \\ \hline
  MS-SUP2 & TC-MS-8 \\ \hline
  MS-SUP3 & TC-MS-9 \\ \hline
  MS-SUP4 & TC-MS-10 \\ \hline
  MS-ADP1 & TC-LF-2 \\ \hline
  MS-ADP2 & TC-LF-2 \\ \hline
  MS-ADP3 & TC-LF-2 \\ \hline
  SR-AR1 & TC-SS-1 \\ \hline
  SR-AR2 & TC-SS-1 \\ \hline
  SR-AR3 & TC-SS-1 \\ \hline
  SR-AR4 & TC-SS-2 \\ \hline
  SR-IR1 & TC-SS-2 \\ \hline
  SR-IR2 & TC-SS-3 \\ \hline
  SR-IR3 & TC-SS-4 \\ \hline
  SR-PR1 & TC-SS-5\\ \hline
  SR-PR2 & TC-SS-6 \\ \hline
  SR-AU1 & TC-SS-6 \\ \hline
  SR-IMR1 & TC-SS-7 \\ \hline
  SR-S1 & TC-SS-8 \\ \hline
  CR-CR1 & TC-CR-1 \\ \hline
  CR-L1 & TC-L-1 \\ \hline
  CR-L2 & TC-L-2 \\ \hline
  CR-S1 & TC-L-3 \\ \hline
  \caption{Requirements to Test Case Traceability Matrix}
\end{longtable}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS
  (detailed design
document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}

\wss{To save space and time, it may be an option to provide less
  detail in this section.
  For the unit tests you can potentially layout your testing strategy
  here.  That is, you
  can explain how tests will be selected for each module.  For
  instance, your test building
  approach could be test cases for each access program, including one
  test for normal behaviour
  and as many tests as needed for edge cases.  Rather than create the
  details of the input
  and output here, you could point to the unit testing code.  For
  this to work, you code
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you
  aren't planning on
  verifying them.  There may also be modules that are part of
  your software, but
  have a lower priority for verification than others.  If this is the case,
explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below
  cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the
  reader how the
tests were selected.}

\begin{enumerate}

  \item{test-id1\\}

    Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
    be automatic}

    Initial State:

    Input:

    Output: \wss{The expected result for the given inputs}

    Test Case Derivation: \wss{Justify the expected value given in
    the Output field}

    How test will be performed:

  \item{test-id2\\}

    Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
    be automatic}

    Initial State:

    Input:

    Output: \wss{The expected result for the given inputs}

    Test Case Derivation: \wss{Justify the expected value given in
    the Output field}

    How test will be performed:

  \item{...\\}

\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
mentioned functional tests.}

\subsubsection{Module ?}

\begin{enumerate}

  \item{test-id1\\}

    Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
    be automatic}

    Initial State:

    Input/Condition:

    Output/Result:

    How test will be performed:

  \item{test-id2\\}

    Type: Functional, Dynamic, Manual, Static etc.

    Initial State:

    Input:

    Output:

    How test will be performed:

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}

\newpage

\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

$\hypertarget{min_age}{MIN\_AGE}$ = 18\\
$\hypertarget{min_age}{MAX\_AGE}$ = 70\\

\subsection{Usability Survey Questions}
\begin{enumerate}
  \item Were you able to understand how to use the system after reading the
    onboarding documentation?
  \item On a scale of 1 to 10, how confident are you using the system?
  \item On a scale of 1 to 10, how easy was it to discover features of the
    system?
  \item How confident were you in interpreting the system feedback? Was it able
    provide confirmation or possible solutions to your input?
  \item Did you find the content of the system to be politically neutral and
    unoffensive?
\end{enumerate}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable?\\
    \\
    When writing this deliverable, our team was able to quickly meet
    and delegate
    sections of the document between members. We had a very productive TA
    meeting for the VnV plan which answered many questions we had. One example
    was we were able to clarify that section 5 of the document would be
    completed later when the design of the system had been done.

  \item What pain points did you experience during this deliverable, and how
    did you resolve them?\\
    \\
    One pain point we experienced was deciding what the best verification and
    validation methods would be. We wanted to ensure that we chose the right
    approaches for this application which would result in the highest quality
    of work. We resolved this challenge by discussing and investigating the
    merits of different options, and developing a test suite which covered
    every requirement we had at least once.

  \item What knowledge and skills will the team collectively need to acquire to
    successfully complete the verification and validation of your project?
    Examples of possible knowledge and skills include dynamic
    testing knowledge,
    static testing knowledge, specific tool usage, Valgrind etc.  You
    should look to
    identify at least one item for each team member.\\
    \\

    Akshit Gulia - Needs to get familiar with the PyTest testing
    framework for python. \\

    Mitchell Hynes - Needs to get familiar with the PyTest testing
    framework for python and Figma for prototyping designs. \\

    Richard Fan - Needs to get familiar with Jest testing framework
    for javascript and PyTest testing framework in python. \\

    Kyle D'Souza - Needs to get familiar with Jest test framework for
    javascript and Figma for prototyping designs. \\

    Rafeed Iqbal - Needs to get familiar with PyTest framework for
    python and GitHub actions for running tests in GitHub. \\

  \item For each of the knowledge areas and skills identified in the previous
    question, what are at least two approaches to acquiring the knowledge or
    mastering the skill?  Of the identified approaches, which will each team
    member pursue, and why did they make this choice?\\
    \\

    For all of the aforementioned skills some potential ways of
    acquiring knowledge or mastering the skill would be to get hands
    on experience with the skill by applying it in a project, reading
    documentation, and watching online tutorials.

    Akshit Gulia - Plans on watching tutorials on youtube related to
    PyTest, and also plans on building a small personal learning
    project on the side in python which uses PyTest for testing. \\

    Mitchell Hynes - Plans to follow the introductory tutorials
    provided by the creators of PyTest and Figma and will also read
    documentation on them. \\

    Richard Fan - Plans to read documentation on Jest and PyTest, and
    also use them in a personal project. \\

    Kyle D'Souza - Plans to watch online tutorials on Jest and Figma.
    Also plans to read documentation on them. \\

    Rafeed Iqbal - Plans to read documentation on GitHub actions and
    Pytest. Also plans to watch online tutorials on them. \\
\end{enumerate}

\end{document}
